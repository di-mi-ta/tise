<!doctype html>
<html lang="en">

<head>
    <base target="_blank">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link rel="stylesheet" href="./css/style.css">


    <title>TISE - Dinh et al.,2021</title>

</head>

<body class="bg-light">
    <div class="container container-small pb-5">
        <div class="row pt-4 pb-3 justify-content-center">
            <div class="col-12 pt-1">
                <h2 class="text-center">TISE: Bag of Metrics for Text-to-Image Synthesis Evaluation</h2>
                <div class="text-center font-weight-normal h5">
                    <a class="pr-5" href="https://di-mi-ta.github.io/">Tan M. Dinh</a>
                    <a class="pr-5" href="https://sites.google.com/site/rangmanhonguyen/">Rang Nguyen</a>
                    <a href="https://sonhua.github.io/">Binh-Son Hua</a>

                    <br>

                    <div class="text-center font-weight-normal h5 pt-2">
                        <span class="pr-5">VinAI Research, Vietnam</span>
                    </div>

                    <div class="text-center font-weight-normal h5 pt-2">
                        <p style="color: #e41d1d" class="pr-5">ECCV 2022</p>
                    </div>

                    <div class="row pt-4 pb-3 justify-content-center">

                        <a class="pr-5" href="https://arxiv.org/abs/2112.01398">
                            <image src="images/paper_logo.png" height="120px">
                                <h5>Paper</h5>
                        </a>

                        <a class="pr-5" href="https://github.com/VinAIResearch/tise-toolbox">
                            <image src="images/github_pad.png" height="120px">
                                <h5>Code</h5>
                        </a>

                    </div>
                </div>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-12">
                <h3>Abstract</h3>
                <p class="text-justify">In this paper, we conduct a study on the state-of-the-art methods for text-to-image synthesis and propose a framework to evaluate these methods. We consider syntheses where an image contains a single or multiple objects. Our study outlines
                    several issues in the current evaluation pipeline: (i) for image quality assessment, a commonly used metric, e.g., Inception Score (IS), is often either miscalibrated for the single-object case or misused for the multi-object case;
                    (ii) for text relevance and object accuracy assessment, there is an overfitting phenomenon in the existing R-precision (RP) and SOA metrics, respectively; (iii) for multi-object case, many vital factors for evaluation, e.g., object
                    fidelity, positional alignment, counting alignment, are largely dismissed; (iv) the ranking of the methods based on current metrics is highly inconsistent with real images. To overcome these issues, we propose a combined set of existing
                    and new metrics to systematically evaluate the methods. For existing metrics, we offer an improved version of IS named IS* by using temperature scaling to calibrate the confidence of the classifier used by IS; we also propose a solution
                    to mitigate the overfitting issues of RP and SOA. For new metrics, we develop counting alignment, positional alignment, object-centric IS, and object-centric FID metrics for evaluating the multi-object case. We show that benchmark
                    with our bag of metrics results in a highly consistent ranking among existing methods, being well-aligned to human evaluation. As a by-product, we create AttnGAN++, a simple but strong baseline for the benchmark by stabilizing the
                    training of AttnGAN using spectral normalization. We also release our toolbox, so-called TISE, for advocating fair and consistent evaluation of text-to-image synthesis models. </p>
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-12">
                <h3>Citation</h3>
            </div>
            <pre>
    @inproceedings{dinh2021tise,
        title={TISE: Bag of Metrics for Text-to-Image Synthesis Evaluation},
        author={Tan M. Dinh and Rang Nguyen and Binh-Son Hua},
        booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
        year={2022}
    }
      </pre>
        </div>

        <!-- Visit tracker -->
        <td>
            <br>
            <div style="display:none;">
                <p>
                    <a href="http://www.clustrmaps.com/map/Di-mi-ta.github.io/tise/" title="Visit tracker for Di-mi-ta.github.io/tise/"><img src="//www.clustrmaps.com/map_v2.png?d=FLXZhOG7_TXhHHd-hyT6sYzzecNsC69wxAWjLYkFL5M" /></a>
                </p>
            </div>
        </td>

</body>

</html>